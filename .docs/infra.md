# Infrastructure

## Core Concepts of a Distributed MapReduce Framework

1. **MapReduce runner:**
    * maybe even in bash.
    * Compiles Map and Reduce functions to go plugins and uploads them to GCS or to worker machines directly (gcloud scp/scp).
    * Uploads input data to GCS.
    * Executes Master and Worker processes on Google Compute Engine (GCE) instances. (gcloud CLI) 
    * **Master input:**
      * master_port (to run Master's RPC server)
    * **Worker input:** 
      * worker_ip (to pass to master for connection)
      * worker_port (to run RPC server and pass to master for connection)
      * master_port (to connect)
      * master_ip (to connect)
      * compiled plugin `.so` file (for Map and Reduce functions)

2. **Master Node:**
    * Assigns Map and Reduce tasks to worker nodes.
    * Tracks the status of tasks (idle, in-progress, completed).
    * Handles worker failures (re-assigns tasks after deadline).

3. **Worker Nodes:**
    * Compile plugins from `plugins/` and load desired plugin to Worker.
    * Execute Map and Reduce tasks assigned by the Master.
    * Read input data for Map tasks (passed by Master).
    * Write intermediate data (output of Map, input to Reduce) (locally or GCS?).
    * Map task does the shuffling (grouping into buckets by key) before writing to intermediate file.
    * Write final output data (output of Reduce).

4. **Input Data:**
    * Text file from `datasets/` dir.

5. **Intermediate Data:**
    * Key-value pairs produced by the Map tasks.
    * Grouped by key into specific index file before being sent to Reduce tasks.

6. **Output Data:**
    * Final results produced by the Reduce tasks.

## Google Cloud Technologies

1. **Google Cloud Storage (GCS):**
    * **TODO**: Check if this is good from the design perspective, Check if this is **EASY TO USE**.
    * **Input Data:** Store large input datasets in GCS buckets.
    * **Intermediate Data:**  Use GCS as temporary storage for intermediate key-value pairs generated by the Map tasks. Worker nodes can write to and read from designated GCS locations.
    * **Output Data:** Store the final output of the Reduce tasks in GCS.

2. **Google Compute Engine (GCE):**
    * **Master Node:** Create a GCE instance to host Master node application.
    * **Worker Nodes:** Create a pool of GCE instances (GCE Managed Instance Group) for Worker nodes.
        * Set up networking to allow communication between the GCE instances.

3. **RPC for communication:**
    * Use Remote Procedure Calls (RPC) for communication between the Master and Worker nodes.
    * Pub/Sub seems like a lot more overhead than necessary for this use case. We can think on using it if we have additional time.

## Simplified Architecture Diagram

```text
                               +-----------------+
                               |   Master Node   | (GCE Instance)
                               |   (Go Program)  |
                               +--------+--------+
                                        |
                                        |  (Task Assignments, Status Updates)
                                        |  (RPC)
             +--------------------------+--------------------------+
             |                          |                          |
+------------+-----------+  +-----------+-----------+  +-----------+-----------+
|       Worker Node      |  |       Worker Node     |  |      Worker Node      | (GCE Instances (Managed Group))
|       (Go Program)     |  |       (Go Program)    |  |      (Go Program)     |
+------------+-----------+  +-----------+-----------+  +-----------+-----------+
             |                          |                          |
             | Read Input               | Write Intermediate       | Read Intermediate
             | Write Intermediate       | Read Intermediate        | Write Output
             v                          v                          v
+--------------------------+--------------------------+--------------------------+
|    Google Cloud Storage  |    Google Cloud Storage  |    Google Cloud Storage  | (Buckets)
|     (Input Data)         |    (Intermediate Data)   |      (Output Data)       |
+--------------------------+--------------------------+--------------------------+
```

## **TODO:**
<!--  -->
- [x] **Set up a Google Cloud Project:** Create a project in the Google Cloud Console.
- [x] **Enable APIs:** Enable the Compute Engine, Cloud Storage, and Pub/Sub APIs (if we choose to use it).
- [ ] **Configure Networking:** open ports and write bash program to run by master to run things on workers.
- [ ] **Write the Code:**
    - [x] Loading Map and Reduce functions from plugins.
    - [ ] GCS interaction for reading/writing data.
    - [x] Task assignment and tracking logic in the Master.
    - [x] Task execution logic in the Workers.
    - [x] Communication between the Master and Workers using RPC.
    - [x] Error handling (also part of previous tasks).
    - [x] Task management (either in-memory or using Pub/Sub).
    - [x] Simple Map and Reduce functions for testing ([mit](https://github.com/zhou-yuhan/MIT-6.824-Distributed-Systems/tree/0c95023d5c36e08880c049d5fbb41c60d34e4299/labs/src/mrapps))
## **Considerations:**

* **Error Handling:** Task retries after deadline.
* **Fault Tolerance:** Handle worker failures gracefully (e.g., the Master should reassign tasks).
* **Intermediate Data Management:**  Efficiently partition, sort, and shuffle intermediate data between the Map and Reduce phases. Distributed file systems or in-memory data grids over GCS?
* **Monitoring and Logging:** Use Google Cloud Monitoring and Logging to track the progress of MapReduce jobs, monitor resource usage, and debug issues.
