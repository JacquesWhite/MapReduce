# Infrastructure

## Core Concepts of a Distributed MapReduce Framework

0. **MapReduce runner:**
    * maybe even in bash.
    * Compiles Map and Reduce functions to go plugins and uploads them to GCS or to worker machines directly (gcloud scp/scp).
    * Uploads input data to GCS.
    * Executes Master and Worker processes on Google Compute Engine (GCE) instances. (gcloud CLI)
        **Master input:** input data location, worker ip's, port
        **Worker input:** master ip, port, function to run

1. **Master Node:**
    * Assigns Map and Reduce tasks to worker nodes.
    * Tracks the status of tasks (idle, in-progress, completed).
    * Handles worker failures (re-assigns tasks after deadline).

2. **Worker Nodes:**
    * Download/load Map and Reduce functions.
    * Execute Map and Reduce tasks assigned by the Master.
    * Read input data for Map tasks.
    * Write intermediate data (output of Map, input to Reduce) (localy or GCS?).
    * Shuffle and sort intermediate data for Reduce tasks.
    * Write final output data (output of Reduce).

3. **Input Data:**
    * IMHO text file/file.

4. **Intermediate Data:**
    * Key-value pairs produced by the Map tasks.
    * Sorted/grouped by key before being sent to Reduce tasks.

5. **Output Data:**
    * Final results produced by the Reduce tasks.

## Google Cloud Technologies

1. **Google Cloud Storage (GCS):**
    * **TODO**: Check if this is good from the design perspective, Check if this is **EASY TO USE**.
    * **Input Data:** Store large input datasets in GCS buckets.
    * **Intermediate Data:**  Use GCS as temporary storage for intermediate key-value pairs generated by the Map tasks. Worker nodes can write to and read from designated GCS locations.
    * **Output Data:** Store the final output of the Reduce tasks in GCS.

2. **Google Compute Engine (GCE):**
    * **Master Node:** Create a GCE instance to host Master node application.
    * **Worker Nodes:** Create a pool of GCE instances (GCE Managed Instance Group) for Worker nodes.
        * Set up networking to allow communication between the GCE instances.

3. **RPC for communication:**
    * Use Remote Procedure Calls (RPC) for communication between the Master and Worker nodes.
    * Pub/Sub seems like a lot more overhead than necessary for this use case. We can think on using it if we have additional time.

## Simplified Architecture Diagram

```text
                               +-----------------+
                               |   Master Node   | (GCE Instance)
                               |   (Go Program)  |
                               +--------+--------+
                                        |
                                        |  (Task Assignments, Status Updates)
                                        |  (RPC)
             +--------------------------+--------------------------+
             |                          |                          |
+------------+-----------+  +-----------+-----------+  +-----------+-----------+
|       Worker Node      |  |       Worker Node     |  |      Worker Node      | (GCE Instances (Managed Group))
|       (Go Program)     |  |       (Go Program)    |  |      (Go Program)     |
+------------+-----------+  +-----------+-----------+  +-----------+-----------+
             |                          |                          |
             | Read Input               | Write Intermediate       | Read Intermediate
             | Write Intermediate       | Read Intermediate        | Write Output
             v                          v                          v
+--------------------------+--------------------------+--------------------------+
|    Google Cloud Storage  |    Google Cloud Storage  |    Google Cloud Storage  | (Buckets)
|     (Input Data)         |    (Intermediate Data)   |      (Output Data)       |
+--------------------------+--------------------------+--------------------------+
```

## Go Implementation Outline **(AI GENERATED BULLSHIT)**

### **[Plugin Loading:](https://github.com/zhou-yuhan/MIT-6.824-Distributed-Systems/blob/0c95023d5c36e08880c049d5fbb41c60d34e4299/labs/src/main/mrworker.go#L34)**

```go
package main

//
// start a worker process, which is implemented
// in ../mr/worker.go. typically there will be
// multiple worker processes, talking to one master.
//
// go run mrworker.go wc.so
//
// Please do not change this file.
//

import "../mr"
import "plugin"
import "os"
import "fmt"
import "log"

func main() {
 if len(os.Args) != 2 {
  fmt.Fprintf(os.Stderr, "Usage: mrworker xxx.so\n")
  os.Exit(1)
 }

 mapf, reducef := loadPlugin(os.Args[1])

 mr.Worker(mapf, reducef)
}

//
// load the application Map and Reduce functions
// from a plugin file, e.g. ../mrapps/wc.so
//
func loadPlugin(filename string) (func(string, string) []mr.KeyValue, func(string, []string) string) {
 p, err := plugin.Open(filename)
 if err != nil {
  log.Fatalf("cannot load plugin %v", filename)
 }
 xmapf, err := p.Lookup("Map")
 if err != nil {
  log.Fatalf("cannot find Map in %v", filename)
 }
 mapf := xmapf.(func(string, string) []mr.KeyValue)
 xreducef, err := p.Lookup("Reduce")
 if err != nil {
  log.Fatalf("cannot find Reduce in %v", filename)
 }
 reducef := xreducef.(func(string, []string) string)

 return mapf, reducef
}
```

### **Master Node (master.go):**

```go
package main

import (
 "context"
 "fmt"
 "log"
 "sync"
 "time"

 "cloud.google.com/go/pubsub"
 "cloud.google.com/go/storage"
)

// ... (Define data structures for Tasks, Workers, etc.)

func main() {
 // 1. Initialize GCS client
 ctx := context.Background()
 storageClient, err := storage.NewClient(ctx)
 if err != nil {
  log.Fatalf("Failed to create storage client: %v", err)
 }
 defer storageClient.Close()

 // 2. (Optional) Initialize Pub/Sub client
 pubsubClient, err := pubsub.NewClient(ctx, "gcp-project-id")
 if err != nil {
  log.Fatalf("Failed to create pubsub client: %v", err)
 }
 defer pubsubClient.Close()

 // 3. Read Input Data (split into chunks)
 inputChunks := readInputDataFromGCS(storageClient, "input-bucket", "input-prefix")

 // 4. Create Task Queues (either in-memory or Pub/Sub topics)
 mapTasks := make(chan Task, len(inputChunks))
 reduceTasks := make(chan Task, numReducers)

 // 5. Start Worker Pool
 var wg sync.WaitGroup
 for i := 0; i < numWorkers; i++ {
  wg.Add(1)
  go startWorker(ctx, &wg, i, mapTasks, reduceTasks, storageClient, pubsubClient) // Pass clients
 }

 // 6. Assign Map Tasks
 for _, chunk := range inputChunks {
  mapTasks <- Task{Type: "map", Input: chunk}
 }

 // 7. Monitor Task Completion (using channels, Pub/Sub, or a shared data store)
 // ... (Logic to track completed Map tasks and trigger Reduce tasks)

 // 8. Assign Reduce Tasks
 // ... (Logic to assign Reduce tasks based on intermediate data locations)

 // 9. Wait for all tasks to complete
 close(mapTasks)
 close(reduceTasks)
 wg.Wait()

 fmt.Println("MapReduce job completed!")
}

// ... (Helper functions for reading/writing GCS, managing tasks, etc.)
```

### **Worker Node (worker.go):**

```go
package main

import (
 "context"
 "fmt"
 "log"
 "sync"

 "cloud.google.com/go/pubsub"
 "cloud.google.com/go/storage"
)

// ... (Define data structures for Tasks, etc.)

func startWorker(ctx context.Context, wg *sync.WaitGroup, workerID int, mapTasks chan Task, reduceTasks chan Task, storageClient *storage.Client, pubsubClient *pubsub.Client) {
 defer wg.Done()

 for {
  select {
  case task, ok := <-mapTasks:
   if !ok {
    return // Map tasks channel closed
   }
   fmt.Printf("Worker %d: Received Map task: %v\n", workerID, task)
   err := processMapTask(ctx, storageClient, task)
   if err != nil {
    log.Printf("Worker %d: Map task failed: %v", workerID, err)
    // Handle error (e.g., report failure to Master)
   } else {
    // Report success to Master (e.g., via Pub/Sub)
   }

  case task, ok := <-reduceTasks:
   if !ok {
    return // Reduce tasks channel closed
   }
   fmt.Printf("Worker %d: Received Reduce task: %v\n", workerID, task)
   err := processReduceTask(ctx, storageClient, task)
   if err != nil {
    log.Printf("Worker %d: Reduce task failed: %v", workerID, err)
    // Handle error
   } else {
    // Report success
   }

  case <-ctx.Done():
   return // Context canceled (e.g., shutdown signal)
  }
 }
}

func processMapTask(ctx context.Context, storageClient *storage.Client, task Task) error {
 // 1. Read input data from GCS (task.Input will contain the chunk location)
 // 2. Perform Map operation (user-defined Map function)
 // 3. Write intermediate data to GCS (partitioned by key for Reduce tasks)
 // ...
 return nil
}

func processReduceTask(ctx context.Context, storageClient *storage.Client, task Task) error {
 // 1. Read intermediate data from GCS (task.Input will contain the intermediate data locations)
 // 2. Perform Reduce operation (user-defined Reduce function)
 // 3. Write final output to GCS
 // ...
 return nil
}

// ... (User-defined Map and Reduce functions)
```

## **TODO:**
<!--  -->
- [x] **Set up a Google Cloud Project:** Create a project in the Google Cloud Console.
- [x] **Enable APIs:** Enable the Compute Engine, Cloud Storage, and Pub/Sub APIs (if we choose to use it).
- [ ] **Configure Netwirking:** open ports and write bash program to run by master to run things on workers.
- [ ] **Write the Code:**
    - [ ] Loading Map and Reduce functions from plugins.
    - [ ] GCS interaction for reading/writing data.
    - [ ] Task assignment and tracking logic in the Master.
    - [ ] Task execution logic in the Workers.
    - [ ] Communication between the Master and Workers using RPC.
    - [ ] Error handling (also part of previous tasks).
    - [ ] Task management (either in-memory or using Pub/Sub).
    - [ ] Simple Map and Reduce functions for testing ([mit](https://github.com/zhou-yuhan/MIT-6.824-Distributed-Systems/tree/0c95023d5c36e08880c049d5fbb41c60d34e4299/labs/src/mrapps))
- [ ] move script for proto generation to the right place.
## **Considerations:**

* **Error Handling:** Task retries after deadline.
* **Fault Tolerance:** Handle worker failures gracefully (e.g., the Master should reassign tasks).
* **Intermediate Data Management:**  Efficiently partition, sort, and shuffle intermediate data between the Map and Reduce phases. Distributed file systems or in-memory data grids over GCS?
* **Monitoring and Logging:** Use Google Cloud Monitoring and Logging to track the progress of MapReduce jobs, monitor resource usage, and debug issues.
